{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syncnet Implementation\n",
    "\n",
    "https://github.com/voletiv/syncnet-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2, os, sys, numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import speechpy\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mouth detection from https://github.com/voletiv/lipreading-in-the-wild-experiments/tree/master/process-lrw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from process_lrw_functions import detect_mouth_in_frame, extract_audio_from_mp4\n",
    "from syncnet_functions import load_pretrained_syncnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_video_input(video):\n",
    "\n",
    "\tdetector = dlib.get_frontal_face_detector()\n",
    "\tpredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "\tcap \t\t= cv2.VideoCapture(video)\n",
    "\tframeFPS \t= int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\tframeCount \t= int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\tframeWidth \t= int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\tframeHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\t# print(\"FPS: {}\".format(frameFPS))\n",
    "\t# print(\"Frames: {}\".format(frameCount))\n",
    "\t# print(\"Width: {}\".format(frameWidth))\n",
    "\t# print(\"Height: {}\".format(frameHeight))\n",
    "\t\n",
    "\tface = dlib.rectangle(30, 30, 220, 220)\n",
    "\t\n",
    "\tlip_model_input = []\n",
    "\n",
    "\t\n",
    "\twhile(cap.isOpened()):\n",
    "\n",
    "\t\t# If frames are extracted from video, all frames are read\n",
    "\t\tframes = []\n",
    "\t\tfor i in range(5):\n",
    "\t\t\t_, frame \t= cap.read()\n",
    "\t\t\tif(frame is None):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tmouth, face = detect_mouth_in_frame(\n",
    "\t\t\t\tframe, detector, predictor,\n",
    "\t\t\t\tprevFace=face,\n",
    "\t\t\t\tverbose=False)\n",
    "\n",
    "\t\t\tmouth = cv2.cvtColor(mouth, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "\t\t\tmouth = cv2.resize( mouth, (112,112))\n",
    "\t\t\t# mouth = mouth[:, :,0] \t# drop the RGB channel\n",
    "\t\t\tframes.append(mouth)\n",
    "\n",
    "\t\t\n",
    "\t\tif(len(frames)==0):\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tstacked = np.stack(frames ,axis=2)\t#syncnet requires (112,112,5)\n",
    "\t\t# input(stacked.shape)\n",
    "\t\tlip_model_input.append(stacked)\n",
    "\t\n",
    "\treturn np.asarray(lip_model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC code thanks to michiyosony \n",
    "\n",
    "https://github.com/voletiv/syncnet-in-keras/issues/1#issuecomment-380149724\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EACH_MFCC_OUTPUT_FRAME_SIZE = 20\n",
    "\n",
    "def extract_mfcc_series(wav_file, target_dir=None):\n",
    "    (rate, sig) = wav.read(wav_file)\n",
    "\n",
    "    try:\n",
    "        mfcc_feat = speechpy.feature.mfcc(sig, sampling_frequency=rate, frame_length=0.010, frame_stride=0.01)\n",
    "    except IndexError:\n",
    "        print(\"index error occurred while extracting mfcc\")\n",
    "        return\n",
    "    print('sample_rate: {}, mfcc_feat length: {}, mfcc_feat[0] length: {}'.format(rate, len(mfcc_feat), len(mfcc_feat[0])))\n",
    "    num_output = len(mfcc_feat) / EACH_MFCC_OUTPUT_FRAME_SIZE\n",
    "    num_output += 1 if (len(mfcc_feat) % EACH_MFCC_OUTPUT_FRAME_SIZE > 0) else 0\n",
    "    \n",
    "    # print(mfcc_feat.shape)\n",
    "    # input(int(num_output))\n",
    "    images = []\n",
    "\n",
    "    for index in range(int(num_output)):\n",
    "        img = Image.new('RGB', (20, 13), \"black\")\n",
    "        pixels = img.load()\n",
    "        for i in range(img.size[0]):\n",
    "            for j in range(img.size[1]):\n",
    "                frame_index = index * EACH_MFCC_OUTPUT_FRAME_SIZE + i\n",
    "                # print(frame_index)\n",
    "                try:\n",
    "                    if mfcc_feat[frame_index][j] < 0:\n",
    "                        red_amount = min(255, 255 * (mfcc_feat[frame_index][j] / -20))\n",
    "                        pixels[i, j] = (int(red_amount), 0, 0)\n",
    "                    elif (mfcc_feat[frame_index][j] > 0):\n",
    "                        blue_amount = min(255, 255 * (mfcc_feat[frame_index][j] / 20))\n",
    "                        pixels[i, j] = (0, 0, int(blue_amount))\n",
    "                except IndexError:\n",
    "                    print(\"index error occurred while extracting mfcc @ \" + str(frame_index) + \",\" + str(j))\n",
    "                    break\n",
    "        # img.save(\"{}/mfcc_{:03d}.png\".format(target_dir, index), 'PNG')\n",
    "        \n",
    "        img_to_np = np.array(img)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Expand an axis\n",
    "        gray_image_exp = np.expand_dims(gray_image, axis=-1)\n",
    "\n",
    "        # img_to_np = img_to_np[:,:,0]\n",
    "        # print(img_to_np.shape)\n",
    "\n",
    "        images.append(gray_image_exp)\n",
    "\n",
    "    return np.asarray(images)\n",
    "\n",
    "\n",
    "def get_audio_input(video):\n",
    "\n",
    "\taudio_out = \"{}.wav\".format(video)\n",
    "\tcmd=\"ffmpeg -y -loglevel panic -i {} -acodec pcm_s16le -ac 1 -ar 16000 {}\".format(video, audio_out)\n",
    "\tos.system(cmd)\n",
    "\treturn extract_mfcc_series(audio_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below - For some reason, i get this error on different videos\n",
    "\n",
    "> ValueError: could not broadcast input array from shape (112,112,5) into shape (112,112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 112, 112, 5)\n"
     ]
    }
   ],
   "source": [
    "lip_input = get_video_input(\"test.mp4\")\n",
    "print(lip_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_rate: 16000, mfcc_feat length: 768, mfcc_feat[0] length: 13\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "src is not a numpy array, neither a scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-6e7d9745fbbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maudio_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_audio_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test.mp4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-0a7e25c0a7a1>\u001b[0m in \u001b[0;36mget_audio_input\u001b[1;34m(video)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mcmd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ffmpeg -y -loglevel panic -i {} -acodec pcm_s16le -ac 1 -ar 16000 {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mextract_mfcc_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-0a7e25c0a7a1>\u001b[0m in \u001b[0;36mextract_mfcc_series\u001b[1;34m(wav_file, target_dir)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Convert to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mgray_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Expand an axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: src is not a numpy array, neither a scalar"
     ]
    }
   ],
   "source": [
    "audio_input = get_audio_input(\"test.mp4\")\n",
    "print(audio_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "version = 'v4'\n",
    "mode = 'both'\n",
    "syncnet_audio_model, syncnet_lip_model = load_pretrained_syncnet_model(version=version, mode=mode, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(syncnet_audio_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(syncnet_lip_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(len(outputs))\n",
    "\n",
    "def get_layer_by_name(the_model, layer_name):\n",
    "    return_layer =None\n",
    "    for layer in the_model.layers:\n",
    "        config = layer.get_config()\n",
    "        if(config[\"name\"] is layer_name):\n",
    "            return_layer = layer\n",
    "    return return_layer\n",
    "\n",
    "\n",
    "def euclidian_distance(np_data_1, np_data_2):\n",
    "    return np.sqrt(np.sum(np.square(np.subtract(np_data_1, np_data_2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model layer output\n",
    "\n",
    "https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def get_layer_output(model, layer, input_data):\n",
    "    inp = model.input     \n",
    "    output = layer.output\n",
    "    layer_fcn = K.function([inp]+ [K.learning_phase()], [ output ])\n",
    "    return layer_fcn([ input_data , 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate embedding Euclidian distance to see if video / audio is synced\n",
    "\n",
    "1. Pass the audio frame through the audio model to get its encoding (a 256-dimensional feature), pass the video frame through the lip model to get its encoding (a 256-dimensional features)\n",
    "\n",
    "2. Check the euclidean distance between the audio encoding and the video encoding.\n",
    "\n",
    "3. If the distance is greater than a threshold (say, 0.6), then it is said the audio and video are not in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (39, 13, 20) for Tensor 'conv1_audio_input:0', which has shape '(?, 13, 20, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-3868f8994733>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0maudio_256d_encoding_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_layer_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyncnet_audio_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fc6_audio\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maudio_256d_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_layer_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyncnet_audio_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_256d_encoding_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-11a529517b07>\u001b[0m in \u001b[0;36mget_layer_output\u001b[1;34m(model, layer, input_data)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlayer_fcn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_fcn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0minput_data\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    973\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    976\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (39, 13, 20) for Tensor 'conv1_audio_input:0', which has shape '(?, 13, 20, 1)'"
     ]
    }
   ],
   "source": [
    "audio_256d_encoding_layer = get_layer_by_name(syncnet_audio_model, \"fc6_audio\")\n",
    "audio_256d_encoding = get_layer_output(syncnet_audio_model, audio_256d_encoding_layer, audio_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lip_256d_encoding_layer= get_layer_by_name(syncnet_lip_model, \"fc6_lip\")\n",
    "lip_256d_encoding = get_layer_output(syncnet_lip_model, lip_256d_encoding_layer, lip_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distace = euclidian_distance(audio_256d_encoding, lip_256d_encoding)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# audio_prediction = syncnet_audio_model.predict(audio_input)\n",
    "# lip_prediction = syncnet_lip_model.predict(lip_input)\n",
    "\n",
    "# print(audio_prediction)\n",
    "# input(\">\")\n",
    "# print(lip_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
